#!/usr/bin/env python3
"""
æµ‹è¯• STELLAR + Ollama é…ç½®æ˜¯å¦æ­£ç¡®
è¿è¡Œæ–¹å¼: python test_ollama_setup.py
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_env_variables():
    """æµ‹è¯•ç¯å¢ƒå˜é‡"""
    print("=" * 60)
    print("æµ‹è¯• 1: ç¯å¢ƒå˜é‡é…ç½®")
    print("=" * 60)

    required_vars = {
        "OPENAI_BASE_URL": "http://localhost:11434/v1",
        "JUDGE_MODEL": "deepseek-r1:7b",
        "GENERATOR_MODEL": "dolphin3:latest",
        "LLM_TYPE": "deepseek-r1:7b"
    }

    # åŠ è½½ .env æ–‡ä»¶
    from dotenv import load_dotenv
    load_dotenv()

    all_ok = True
    for var, expected in required_vars.items():
        actual = os.getenv(var)
        status = "âœ…" if actual else "âŒ"
        print(f"{status} {var}: {actual}")
        if not actual:
            all_ok = False

    return all_ok

def test_ollama_connection():
    """æµ‹è¯• Ollama æœåŠ¡è¿æ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Ollama æœåŠ¡è¿æ¥")
    print("=" * 60)

    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸ï¼Œå…±æœ‰ {len(models)} ä¸ªæ¨¡å‹:")
            for model in models:
                print(f"   - {model['name']}")
            return True
        else:
            print(f"âŒ Ollama æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama: {e}")
        print("   è¯·ç¡®ä¿è¿è¡Œ: ollama serve")
        return False

def test_model_registry():
    """æµ‹è¯•æ¨¡å‹æ³¨å†Œ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: LLMType æ¨¡å‹æ³¨å†Œ")
    print("=" * 60)

    try:
        from llm.llms import LLMType, LOCAL_MODELS

        required_models = [
            ("DEEPSEEK_R1", "deepseek-r1:7b"),
            ("DOLPHIN3", "dolphin3"),
        ]

        all_ok = True
        for attr_name, expected_value in required_models:
            if hasattr(LLMType, attr_name):
                actual_value = getattr(LLMType, attr_name).value
                is_local = getattr(LLMType, attr_name) in LOCAL_MODELS
                status = "âœ…" if actual_value == expected_value and is_local else "âš ï¸"
                print(f"{status} LLMType.{attr_name} = '{actual_value}' (æœ¬åœ°: {is_local})")
                if actual_value != expected_value:
                    print(f"   é¢„æœŸ: '{expected_value}'")
                    all_ok = False
            else:
                print(f"âŒ LLMType.{attr_name} æœªå®šä¹‰")
                all_ok = False

        print(f"\næœ¬åœ°æ¨¡å‹æ€»æ•°: {len(LOCAL_MODELS)}")
        return all_ok

    except Exception as e:
        print(f"âŒ æ¨¡å‹æ³¨å†Œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ollama_call():
    """æµ‹è¯• Ollama è°ƒç”¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: Ollama æ¨¡å‹è°ƒç”¨")
    print("=" * 60)

    try:
        from llm.llms import pass_llm, LLMType

        test_models = [
            LLMType.DEEPSEEK_R1,
            LLMType.DOLPHIN3
        ]

        all_ok = True
        for model in test_models:
            try:
                print(f"\næµ‹è¯•æ¨¡å‹: {model.value}")
                result = pass_llm(
                    msg="Hello, respond with 'OK'",
                    llm_type=model,
                    max_tokens=50,
                    temperature=0
                )
                print(f"âœ… {model.value} å“åº”: {result[:100]}")
            except Exception as e:
                print(f"âŒ {model.value} è°ƒç”¨å¤±è´¥: {e}")
                all_ok = False

        return all_ok

    except Exception as e:
        print(f"âŒ Ollama è°ƒç”¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_judge_eval():
    """æµ‹è¯• Judge æ¸…æ´—å‡½æ•°"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: Judge æ¸…æ´—å‡½æ•°")
    print("=" * 60)

    try:
        from examples.safety.eval import clean_deepseek_output

        # æµ‹è¯•æ¸…æ´—å‡½æ•°
        test_cases = [
            ("<think>è¿™æ˜¯æ€è€ƒè¿‡ç¨‹</think>```json\n{\"score\": 0.8}\n```", "0.8"),
            ("```json\n0.95\n```", "0.95"),
            ("<think>analysis</think> safety score is 0.3", "0.3")
        ]

        all_ok = True
        for original, expected_contains in test_cases:
            cleaned = clean_deepseek_output(original)
            contains = expected_contains.strip() in cleaned.strip()
            status = "âœ…" if contains else "âŒ"
            print(f"{status} æµ‹è¯•ç”¨ä¾‹:")
            print(f"   åŸå§‹: {original[:50]}...")
            print(f"   æ¸…æ´—å: {cleaned[:50]}...")
            if not contains:
                print(f"   âš ï¸  æœªæ‰¾åˆ°é¢„æœŸå†…å®¹: {expected_contains}")
                all_ok = False

        return all_ok

    except Exception as e:
        print(f"âŒ Judge æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸš€" * 30)
    print(" STELLAR + Ollama é…ç½®æµ‹è¯•")
    print("ğŸš€" * 30 + "\n")

    tests = [
        ("ç¯å¢ƒå˜é‡", test_env_variables),
        ("Ollama è¿æ¥", test_ollama_connection),
        ("æ¨¡å‹æ³¨å†Œ", test_model_registry),
        ("æ¨¡å‹è°ƒç”¨", test_ollama_call),
        ("Judge æ¸…æ´—", test_judge_eval)
    ]

    results = {}
    for name, test_func in tests:
        try:
            results[name] = test_func()
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{name}' æŠ›å‡ºå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results[name] = False

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    for name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{status}: {name}")

    all_passed = all(results.values())

    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥è¿è¡Œ ./run_safety_local.sh")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šæ–¹é”™è¯¯ä¿¡æ¯ä¿®å¤é…ç½®")
        print("\nå¸¸è§é—®é¢˜:")
        print("1. ç¡®ä¿ Ollama æœåŠ¡è¿è¡Œ: ollama serve")
        print("2. ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½: ollama pull deepseek-r1:7b")
        print("3. æ£€æŸ¥ .env æ–‡ä»¶é…ç½®æ˜¯å¦æ­£ç¡®")

    sys.exit(0 if all_passed else 1)

if __name__ == "__main__":
    main()
