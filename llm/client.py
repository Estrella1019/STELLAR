# llm/client.py

import os
import time
import traceback
import re
import json
import httpx
from openai import OpenAI, BadRequestError, APITimeoutError
from llm.llms import LLMType

from dotenv import load_dotenv
load_dotenv()

class OpenAIClient:
    total_token_usage = 0
    total_call_count = 0
    
    def __init__(self, model_enum: LLMType, api_key: str = None):
        # 1. å…¼å®¹æ€§å¤„ç†ï¼šæ— è®ºä¼ å…¥çš„æ˜¯ Enum è¿˜æ˜¯ å­—ç¬¦ä¸²ï¼Œéƒ½è½¬ä¸ºå­—ç¬¦ä¸²
        raw_model_name = model_enum.value if hasattr(model_enum, 'value') else str(model_enum)
        self.model_name = raw_model_name

        # 2. æ¨¡å‹é‡å®šå‘ (ä» .env è¯»å–é…ç½®)
        judge_model = os.getenv("JUDGE_MODEL", "deepseek-r1:7b")
        generator_model = os.getenv("GENERATOR_MODEL", "dolphin3:latest")
        
        # æ‹¦æˆª STELLAR é»˜è®¤çš„ GPT æ¨¡å‹ï¼Œè½¬ä¸ºæœ¬åœ°æ¨¡å‹
        if "gpt-4" in raw_model_name:
            if "mini" in raw_model_name:
                self.deployment_name = judge_model  # åˆ¤å·æ¨¡å‹
            else:
                self.deployment_name = generator_model # ç”Ÿæˆæ¨¡å‹
            print(f"ğŸ”„ [Redirect] {raw_model_name} => {self.deployment_name}")
        else:
            self.deployment_name = raw_model_name
            print(f"ğŸ‘€ [Init] Client: {self.deployment_name}")

        # 3. åˆå§‹åŒ–å®¢æˆ·ç«¯ (Ollama)
        # âš ï¸ è®¾ç½® 600ç§’ (10åˆ†é’Ÿ) è¶…æ—¶ï¼Œé˜²æ­¢ R1 æ€è€ƒæ—¶é—´è¿‡é•¿å¯¼è‡´æ–­å¼€
        self.client = OpenAI(
            api_key="ollama",
            base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:11434/v1"),
            timeout=httpx.Timeout(600.0, read=600.0, write=600.0, connect=10.0)
        )

        self.token_usage = 0
        self.call_counter = 0

    def _clean_deepseek_response(self, content):
        """
        [æ ¸å¿ƒä¿®å¤] æš´åŠ›æå– JSONã€‚
        å³ä½¿æ¨¡å‹è¾“å‡ºäº† <think> æˆ– 'Here is the json:', ä¹Ÿèƒ½æå–å‡ºæ­£ç¡®çš„ {...} éƒ¨åˆ†ã€‚
        """
        if not content:
            return ""
        
        # 1. å…ˆå»æ‰ <think> æ ‡ç­¾
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        
        # 2. å°è¯•å¯»æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' åŒ…è£¹çš„å†…å®¹
        #    è¿™èƒ½è§£å†³ "Sure! ```json { ... } ```" è¿™ç§æ ¼å¼é—®é¢˜
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            clean_json = match.group(1)
            return clean_json
            
        # 3. å¦‚æœæ‰¾ä¸åˆ°å¤§æ‹¬å·ï¼Œåªèƒ½è¿”å›åŸå§‹å†…å®¹ (å¯èƒ½ä¼šå¯¼è‡´å¤–éƒ¨è§£æå¤±è´¥)
        return content.strip()

    def call(self, 
             prompt: str, 
             max_tokens=4096, 
             temperature=0.6, 
             system_message=None, 
             context=None):
        
        if system_message is None:
            system_message = "You are a helpful assistant."

        try:        
            self.call_counter += 1
            OpenAIClient.total_call_count += 1

            start_time = time.time()
            formatted_system_msg = system_message.format(context) if context else system_message
            
            # DeepSeek R1 æ¨èç”¨æ³•ï¼šSystem Prompt æ”¾å…¥ user æ¶ˆæ¯å‰æˆ– system æ¶ˆæ¯
            messages = [
                {"role": "system", "content": formatted_system_msg},
                {"role": "user", "content": prompt}
            ]

            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            end_time = time.time()

            used_tokens = 0
            if response.usage:
                used_tokens = response.usage.total_tokens
            self.token_usage += used_tokens
            OpenAIClient.total_token_usage += used_tokens

            raw_content = response.choices[0].message.content
            
            # [è°ƒç”¨æ¸…æ´—å‡½æ•°]
            clean_content = self._clean_deepseek_response(raw_content)

            # [Debug] å¦‚æœè¿”å›ä¸ºç©ºï¼Œæ‰“å°è­¦å‘Š
            if not clean_content:
                print(f"âš ï¸ [Client] Empty response from {self.deployment_name}")

            return clean_content, used_tokens, end_time - start_time

        except APITimeoutError:
            print(f"âŒ [Timeout] Model {self.deployment_name} did not reply in 600s.")
            return "", 0, -1
            
        except BadRequestError as e:
            print(f"âŒ [BadRequest] {e}")
            return f"BADREQUEST_ERROR: {e}", 0, -1

        except Exception as e:
            print(f"âŒ [Exception] {type(e).__name__}: {e}")
            traceback.print_exc()
            return "", 0, -1
    
    @classmethod
    def from_deployment_name(cls, deployment_name: str, api_key: str = None):
        class MockEnum:
            value = deployment_name
        return cls(MockEnum, api_key="ollama")