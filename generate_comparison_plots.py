#!/usr/bin/env python3
"""
ç”Ÿæˆ STELLAR è®ºæ–‡é£æ ¼çš„å¯¹æ¯”å›¾è¡¨
åŒ…å« TSR (Test Success Rate), Time, Critical Ratio ç­‰å…³é”®æŒ‡æ ‡
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 300
sns.set_style("whitegrid")
sns.set_palette("husl")

class ExperimentAnalyzer:
    """å®éªŒç»“æœåˆ†æå™¨"""

    def __init__(self, results_base_path="results"):
        self.results_base = Path(results_base_path)
        self.experiments = {
            "RANDOM": None,
            "T-wise": None,
            "STELLAR": None
        }

    def find_latest_results(self):
        """æŸ¥æ‰¾æœ€æ–°çš„å®éªŒç»“æœ"""
        print("ğŸ“ æœç´¢å®éªŒç»“æœ...")

        # æŸ¥æ‰¾æ‰€æœ‰ç»“æœç›®å½•
        all_dirs = list(self.results_base.glob("**/summary_results.csv"))

        if not all_dirs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
            return False

        # æŒ‰å®éªŒç±»å‹åˆ†ç»„ - ä½¿ç”¨å®Œæ•´è·¯å¾„åˆ¤æ–­
        for result_file in all_dirs:
            result_dir = result_file.parent
            full_path = str(result_dir)

            # åˆ¤æ–­å®éªŒç±»å‹ - ä¼˜å…ˆæ£€æŸ¥çˆ¶ç›®å½•å
            exp_type = None

            # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦åŒ…å« random/twise/stellar æ–‡ä»¶å¤¹
            if "/random/" in full_path:
                exp_type = "RANDOM"
            elif "/twise/" in full_path:
                exp_type = "T-wise"
            elif "/stellar/" in full_path:
                exp_type = "STELLAR"
            # å¦‚æœè·¯å¾„ä¸­æ²¡æœ‰æ˜ç¡®æ–‡ä»¶å¤¹ï¼Œæ£€æŸ¥ç®—æ³•ç±»å‹
            elif "NSGA" in full_path or "nsga" in full_path.lower():
                exp_type = "STELLAR"
            elif "GS" in full_path or "_gs_" in full_path.lower():
                exp_type = "T-wise"
            elif "RS" in full_path or "_rs_" in full_path.lower():
                # ç¡®ä¿ä¸æ˜¯ NSGA-II çš„ RS å­æ–‡ä»¶å¤¹
                if "NSGA" not in full_path and "nsga" not in full_path.lower():
                    exp_type = "RANDOM"

            if exp_type is None:
                continue

            # ä¿å­˜æœ€æ–°çš„ç»“æœï¼ˆå¦‚æœå·²æœ‰ç»“æœï¼Œæ¯”è¾ƒæ—¶é—´æˆ³é€‰æ‹©æœ€æ–°çš„ï¼‰
            if self.experiments[exp_type] is None:
                self.experiments[exp_type] = result_dir
                print(f"  âœ… æ‰¾åˆ° {exp_type}: {result_dir.relative_to(self.results_base)}")
            else:
                # æ¯”è¾ƒæ—¶é—´æˆ³ï¼Œé€‰æ‹©æœ€æ–°çš„
                current_time = result_dir.name  # ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¶é—´æˆ³
                existing_time = self.experiments[exp_type].name
                if current_time > existing_time:
                    self.experiments[exp_type] = result_dir
                    print(f"  âœ… æ›´æ–° {exp_type}: {result_dir.relative_to(self.results_base)}")

        return any(v is not None for v in self.experiments.values())

    def load_experiment_data(self, exp_type):
        """åŠ è½½å•ä¸ªå®éªŒçš„æ•°æ®"""
        result_dir = self.experiments[exp_type]
        if result_dir is None:
            return None

        data = {
            "type": exp_type,
            "dir": result_dir,
            "summary": None,
            "failures": None,
            "metrics": {}
        }

        # è¯»å– summary_results.csv
        summary_file = result_dir / "summary_results.csv"
        if summary_file.exists():
            data["summary"] = pd.read_csv(summary_file)

        # è¯»å– failures_over_time.csv
        failures_file = result_dir / "failures_over_time.csv"
        if failures_file.exists():
            data["failures"] = pd.read_csv(failures_file)

        # è¯»å– calculation_properties.csv
        calc_file = result_dir / "calculation_properties.csv"
        if calc_file.exists():
            calc_df = pd.read_csv(calc_file)
            for _, row in calc_df.iterrows():
                data["metrics"][row.iloc[0]] = row.iloc[1]

        # è®¡ç®—å…³é”®æŒ‡æ ‡
        self._calculate_metrics(data)

        return data

    def _calculate_metrics(self, data):
        """è®¡ç®—å…³é”®æŒ‡æ ‡"""
        metrics = data["metrics"]
        summary = data["summary"]
        failures = data["failures"]

        # TSR (Test Success Rate) = å¤±è´¥æµ‹è¯•æ•° / æ€»æµ‹è¯•æ•°
        if failures is not None and len(failures) > 0:
            total_failures = failures["FailuresFound"].iloc[-1]
            metrics["total_failures"] = int(total_failures)

            # é¦–ä¸ªå¤±è´¥å‘ç°æ—¶é—´
            first_failure_idx = failures[failures["FailuresFound"] > 0].index[0] if any(failures["FailuresFound"] > 0) else -1
            if first_failure_idx >= 0:
                metrics["first_failure_time"] = float(failures.loc[first_failure_idx, "Time_s"])
            else:
                metrics["first_failure_time"] = 0
        else:
            metrics["total_failures"] = 0
            metrics["first_failure_time"] = 0

        # ä» summary ä¸­æå–
        if summary is not None and len(summary) > 0:
            last_row = summary.iloc[-1]

            # æ€»æµ‹è¯•æ•°
            if "n_evaluations" in summary.columns:
                metrics["total_tests"] = int(last_row["n_evaluations"])
            elif "evaluated_individuals" in summary.columns:
                metrics["total_tests"] = int(last_row["evaluated_individuals"])
            else:
                metrics["total_tests"] = 100  # é»˜è®¤å€¼

            # æ‰§è¡Œæ—¶é—´
            if "exec_time" in summary.columns:
                metrics["execution_time"] = float(last_row["exec_time"])
            else:
                metrics["execution_time"] = 0

            # å»é‡åçš„å¤±è´¥æ•°
            if "Number Critical Scenarios (duplicate free)" in summary.columns:
                metrics["unique_failures"] = int(last_row["Number Critical Scenarios (duplicate free)"])
            else:
                metrics["unique_failures"] = metrics["total_failures"]

            # å¹³å‡é€‚åº”åº¦å’Œæœ€ä½³é€‚åº”åº¦
            if "avg_fitness" in summary.columns:
                metrics["avg_fitness"] = float(last_row["avg_fitness"])
            else:
                metrics["avg_fitness"] = 0

            if "best_fitness" in summary.columns:
                metrics["best_fitness"] = float(last_row["best_fitness"])
            else:
                metrics["best_fitness"] = 0

        # è®¡ç®— TSR (Critical Ratio)
        if metrics["total_tests"] > 0:
            metrics["tsr"] = (metrics["total_failures"] / metrics["total_tests"]) * 100
        else:
            metrics["tsr"] = 0

        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        if metrics.get("execution_time", 0) > 0:
            metrics["failures_per_second"] = metrics["total_failures"] / metrics["execution_time"]
            metrics["failures_per_minute"] = metrics["total_failures"] / (metrics["execution_time"] / 60)
            metrics["tests_per_minute"] = metrics["total_tests"] / (metrics["execution_time"] / 60)
        else:
            metrics["failures_per_second"] = 0
            metrics["failures_per_minute"] = 0
            metrics["tests_per_minute"] = 0

        # å¤±è´¥å‘ç°å¤šæ ·æ€§ (å»é‡ç‡)
        if metrics["total_failures"] > 0:
            metrics["diversity_ratio"] = (metrics["unique_failures"] / metrics["total_failures"]) * 100
        else:
            metrics["diversity_ratio"] = 0

        # å¤±è´¥å‘ç°æˆåŠŸç‡
        if metrics["total_tests"] > 0:
            metrics["failure_detection_rate"] = (metrics["total_failures"] / metrics["total_tests"]) * 100
        else:
            metrics["failure_detection_rate"] = 0

    def generate_comparison_plots(self, output_dir="comparison_results"):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        # åŠ è½½æ‰€æœ‰å®éªŒæ•°æ®
        exp_data = {}
        for exp_type in ["RANDOM", "T-wise", "STELLAR"]:
            data = self.load_experiment_data(exp_type)
            if data:
                exp_data[exp_type] = data
                print(f"\nâœ… {exp_type} æ•°æ®:")
                print(f"   æ€»æµ‹è¯•: {data['metrics']['total_tests']}")
                print(f"   å¤±è´¥æ•°: {data['metrics']['total_failures']}")
                print(f"   å»é‡å¤±è´¥: {data['metrics'].get('unique_failures', 'N/A')}")
                print(f"   TSR: {data['metrics']['tsr']:.2f}%")
                print(f"   æ—¶é—´: {data['metrics']['execution_time']:.1f}s ({data['metrics']['execution_time']/60:.1f}åˆ†é’Ÿ)")
                print(f"   é¦–æ¬¡å¤±è´¥æ—¶é—´: {data['metrics'].get('first_failure_time', 'N/A'):.1f}s")
                print(f"   æ•ˆç‡: {data['metrics']['failures_per_minute']:.2f} failures/min")
                print(f"   å¤šæ ·æ€§: {data['metrics'].get('diversity_ratio', 'N/A'):.1f}%")

        if not exp_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å®éªŒæ•°æ®")
            return

        # ç”Ÿæˆå„ç§å›¾è¡¨
        self._plot_tsr_comparison(exp_data, output_path)
        self._plot_time_comparison(exp_data, output_path)
        self._plot_failures_over_time(exp_data, output_path)
        self._plot_efficiency(exp_data, output_path)
        self._plot_comprehensive_comparison(exp_data, output_path)

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(exp_data, output_path)

        print(f"\nâœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆåˆ°: {output_path}")

    def _plot_tsr_comparison(self, exp_data, output_path):
        """å›¾1: TSR (Test Success Rate / Critical Ratio) å¯¹æ¯”"""
        fig, ax = plt.subplots(figsize=(10, 6))

        methods = list(exp_data.keys())
        tsrs = [exp_data[m]["metrics"]["tsr"] for m in methods]
        colors = ["#3498db", "#e74c3c", "#2ecc71"]

        bars = ax.bar(methods, tsrs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%',
                   ha='center', va='bottom', fontsize=14, fontweight='bold')

        ax.set_ylabel('TSR (Test Success Rate) %', fontsize=13, fontweight='bold')
        ax.set_title('Test Success Rate (TSR) Comparison\næ”»å‡»æˆåŠŸç‡å¯¹æ¯”',
                    fontsize=15, fontweight='bold', pad=20)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_ylim(0, max(tsrs) * 1.2)

        plt.tight_layout()
        plt.savefig(output_path / "1_tsr_comparison.png", dpi=300, bbox_inches='tight')
        print(f"  âœ… TSR å¯¹æ¯”å›¾")
        plt.close()

    def _plot_time_comparison(self, exp_data, output_path):
        """å›¾2: æ‰§è¡Œæ—¶é—´å¯¹æ¯”"""
        fig, ax = plt.subplots(figsize=(10, 6))

        methods = list(exp_data.keys())
        times = [exp_data[m]["metrics"]["execution_time"] / 60 for m in methods]  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        colors = ["#3498db", "#e74c3c", "#2ecc71"]

        bars = ax.bar(methods, times, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}m',
                   ha='center', va='bottom', fontsize=14, fontweight='bold')

        ax.set_ylabel('Execution Time (minutes)', fontsize=13, fontweight='bold')
        ax.set_title('Execution Time Comparison\næ‰§è¡Œæ—¶é—´å¯¹æ¯”',
                    fontsize=15, fontweight='bold', pad=20)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(output_path / "2_time_comparison.png", dpi=300, bbox_inches='tight')
        print(f"  âœ… æ—¶é—´å¯¹æ¯”å›¾")
        plt.close()

    def _plot_failures_over_time(self, exp_data, output_path):
        """å›¾3: å¤±è´¥å‘ç°æ›²çº¿ï¼ˆè®ºæ–‡å›¾2é£æ ¼ï¼‰"""
        fig, ax = plt.subplots(figsize=(12, 7))

        colors = {"RANDOM": "#3498db", "T-wise": "#e74c3c", "STELLAR": "#2ecc71"}
        markers = {"RANDOM": "o", "T-wise": "s", "STELLAR": "^"}

        for method, data in exp_data.items():
            if data["failures"] is not None:
                df = data["failures"]
                time_minutes = df["Time_s"] / 60
                failures = df["FailuresFound"]

                ax.plot(time_minutes, failures,
                       label=method,
                       color=colors[method],
                       marker=markers[method],
                       linewidth=2.5,
                       markersize=8,
                       alpha=0.9)

        ax.set_xlabel('Time (minutes)', fontsize=13, fontweight='bold')
        ax.set_ylabel('Number of Failures Found', fontsize=13, fontweight='bold')
        ax.set_title('Failures Discovered Over Time\nå¤±è´¥å‘ç°æ›²çº¿ (è®ºæ–‡å›¾2é£æ ¼)',
                    fontsize=15, fontweight='bold', pad=20)
        ax.legend(fontsize=12, framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(output_path / "3_failures_over_time.png", dpi=300, bbox_inches='tight')
        print(f"  âœ… å¤±è´¥æ›²çº¿å›¾ï¼ˆè®ºæ–‡é£æ ¼ï¼‰")
        plt.close()

    def _plot_efficiency(self, exp_data, output_path):
        """å›¾4: æ•ˆç‡å¯¹æ¯”ï¼ˆå¤±è´¥æ•°/åˆ†é’Ÿï¼‰"""
        fig, ax = plt.subplots(figsize=(10, 6))

        methods = list(exp_data.keys())
        efficiencies = []
        for m in methods:
            time_min = exp_data[m]["metrics"]["execution_time"] / 60
            failures = exp_data[m]["metrics"]["total_failures"]
            eff = failures / time_min if time_min > 0 else 0
            efficiencies.append(eff)

        colors = ["#3498db", "#e74c3c", "#2ecc71"]
        bars = ax.bar(methods, efficiencies, color=colors, alpha=0.8,
                     edgecolor='black', linewidth=1.5)

        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.2f}',
                   ha='center', va='bottom', fontsize=14, fontweight='bold')

        ax.set_ylabel('Failures per Minute', fontsize=13, fontweight='bold')
        ax.set_title('Efficiency Comparison\næ•ˆç‡å¯¹æ¯” (å¤±è´¥æ•°/åˆ†é’Ÿ)',
                    fontsize=15, fontweight='bold', pad=20)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(output_path / "4_efficiency_comparison.png", dpi=300, bbox_inches='tight')
        print(f"  âœ… æ•ˆç‡å¯¹æ¯”å›¾")
        plt.close()

    def _plot_comprehensive_comparison(self, exp_data, output_path):
        """å›¾5: ç»¼åˆå¯¹æ¯”ï¼ˆå¤šæŒ‡æ ‡é›·è¾¾å›¾ï¼‰"""
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

        # å‡†å¤‡æ•°æ®
        categories = ['TSR\n(%)', 'Total\nFailures', 'Efficiency\n(F/min)',
                     'Speed\n(1/time)']
        N = len(categories)

        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]

        colors = {"RANDOM": "#3498db", "T-wise": "#e74c3c", "STELLAR": "#2ecc71"}

        for method, data in exp_data.items():
            metrics = data["metrics"]

            # å½’ä¸€åŒ–æ•°æ®åˆ° 0-100
            tsr_norm = metrics["tsr"]
            failures_norm = (metrics["total_failures"] / max(
                [d["metrics"]["total_failures"] for d in exp_data.values()])) * 100

            time_min = metrics["execution_time"] / 60
            efficiency_norm = ((metrics["total_failures"] / time_min) / max(
                [d["metrics"]["total_failures"] / (d["metrics"]["execution_time"] / 60)
                 for d in exp_data.values()])) * 100 if time_min > 0 else 0

            speed_norm = (1 / (metrics["execution_time"] / 60) / max(
                [1 / (d["metrics"]["execution_time"] / 60)
                 for d in exp_data.values()])) * 100 if time_min > 0 else 0

            values = [tsr_norm, failures_norm, efficiency_norm, speed_norm]
            values += values[:1]

            ax.plot(angles, values, 'o-', linewidth=2.5,
                   label=method, color=colors[method], markersize=8)
            ax.fill(angles, values, alpha=0.15, color=colors[method])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
        ax.set_ylim(0, 100)
        ax.set_title('Comprehensive Performance Comparison\nç»¼åˆæ€§èƒ½å¯¹æ¯”',
                    fontsize=15, fontweight='bold', pad=30)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
        ax.grid(True, linestyle='--', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path / "5_comprehensive_radar.png", dpi=300, bbox_inches='tight')
        print(f"  âœ… ç»¼åˆå¯¹æ¯”é›·è¾¾å›¾")
        plt.close()

    def _generate_report(self, exp_data, output_path):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report = []
        report.append("=" * 120)
        report.append("STELLAR å®éªŒå¯¹æ¯”æŠ¥å‘Š (Extended Metrics)")
        report.append("=" * 120)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # è¡¨æ ¼å¯¹æ¯” - æ‰©å±•ç‰ˆ
        report.append("ğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯” (Key Metrics Comparison):")
        report.append("-" * 120)

        # è¡¨å¤´
        header = f"{'æ–¹æ³•':<12} {'æ€»æµ‹è¯•':<10} {'å¤±è´¥æ•°':<10} {'å»é‡å¤±è´¥':<10} {'TSR(%)':<10} " \
                 f"{'æ—¶é—´(åˆ†)':<10} {'æ•ˆç‡(F/m)':<12} {'æµ‹è¯•/åˆ†':<10}"
        report.append(header)
        report.append("-" * 120)

        # æ•°æ®è¡Œ
        for method, data in exp_data.items():
            m = data["metrics"]
            time_min = m["execution_time"] / 60 if m["execution_time"] > 0 else 0.01

            row = f"{method:<12} {m['total_tests']:<10} {m['total_failures']:<10} " \
                  f"{m.get('unique_failures', m['total_failures']):<10} {m['tsr']:<10.2f} " \
                  f"{time_min:<10.1f} {m['failures_per_minute']:<12.2f} " \
                  f"{m['tests_per_minute']:<10.1f}"
            report.append(row)

        report.append("-" * 120)
        report.append("")

        # è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        report.append("ğŸ“ˆ è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ (Detailed Performance Metrics):")
        report.append("-" * 120)

        header2 = f"{'æ–¹æ³•':<12} {'é¦–æ¬¡å¤±è´¥(s)':<15} {'å¤šæ ·æ€§(%)':<12} {'æ£€æµ‹ç‡(%)':<12} " \
                  f"{'å¹³å‡é€‚åº”åº¦':<15} {'æœ€ä½³é€‚åº”åº¦':<15}"
        report.append(header2)
        report.append("-" * 120)

        for method, data in exp_data.items():
            m = data["metrics"]

            row = f"{method:<12} {m.get('first_failure_time', 0):<15.1f} " \
                  f"{m.get('diversity_ratio', 0):<12.1f} {m.get('failure_detection_rate', 0):<12.2f} " \
                  f"{m.get('avg_fitness', 0):<15.3f} {m.get('best_fitness', 0):<15.3f}"
            report.append(row)

        report.append("-" * 120)
        report.append("")

        # æ”¹è¿›å€æ•°åˆ†æ
        report.append("ğŸ¯ æ”¹è¿›åˆ†æ (Improvement Analysis):")
        report.append("-" * 120)

        if "STELLAR" in exp_data and "RANDOM" in exp_data:
            stellar_failures = exp_data["STELLAR"]["metrics"]["total_failures"]
            random_failures = exp_data["RANDOM"]["metrics"]["total_failures"]
            stellar_tsr = exp_data["STELLAR"]["metrics"]["tsr"]
            random_tsr = exp_data["RANDOM"]["metrics"]["tsr"]

            if random_failures > 0:
                improvement_failures = stellar_failures / random_failures
                improvement_tsr = stellar_tsr / random_tsr if random_tsr > 0 else 0

                report.append(f"STELLAR vs RANDOM:")
                report.append(f"  - å¤±è´¥æ•°æ”¹è¿›: {improvement_failures:.2f}x ({random_failures} â†’ {stellar_failures})")
                report.append(f"  - TSRæ”¹è¿›: {improvement_tsr:.2f}x ({random_tsr:.2f}% â†’ {stellar_tsr:.2f}%)")

                # é¦–æ¬¡å¤±è´¥æ—¶é—´å¯¹æ¯”
                stellar_first = exp_data["STELLAR"]["metrics"].get("first_failure_time", 0)
                random_first = exp_data["RANDOM"]["metrics"].get("first_failure_time", 0)
                if random_first > 0 and stellar_first > 0:
                    speedup = random_first / stellar_first
                    report.append(f"  - é¦–æ¬¡å‘ç°åŠ é€Ÿ: {speedup:.2f}x ({random_first:.1f}s â†’ {stellar_first:.1f}s)")

            if "T-wise" in exp_data:
                twise_failures = exp_data["T-wise"]["metrics"]["total_failures"]
                twise_tsr = exp_data["T-wise"]["metrics"]["tsr"]

                if twise_failures > 0:
                    improvement_twise = stellar_failures / twise_failures
                    improvement_tsr_twise = stellar_tsr / twise_tsr if twise_tsr > 0 else 0

                    report.append(f"\nSTELLAR vs T-wise:")
                    report.append(f"  - å¤±è´¥æ•°æ”¹è¿›: {improvement_twise:.2f}x ({twise_failures} â†’ {stellar_failures})")
                    report.append(f"  - TSRæ”¹è¿›: {improvement_tsr_twise:.2f}x ({twise_tsr:.2f}% â†’ {stellar_tsr:.2f}%)")

        report.append("")
        report.append("=" * 120)

        # æŒ‡æ ‡è¯´æ˜
        report.append("\nğŸ“– æŒ‡æ ‡è¯´æ˜ (Metrics Description):")
        report.append("-" * 120)
        report.append("TSR (Test Success Rate): æ”»å‡»æˆåŠŸç‡ï¼Œå¤±è´¥æ•°å æ€»æµ‹è¯•æ•°çš„ç™¾åˆ†æ¯”")
        report.append("å»é‡å¤±è´¥: å»é™¤é‡å¤åçš„ç‹¬ç‰¹å¤±è´¥åœºæ™¯æ•°é‡")
        report.append("æ•ˆç‡ (F/m): æ¯åˆ†é’Ÿå‘ç°çš„å¤±è´¥æ•°ï¼Œè¶Šé«˜è¶Šå¥½")
        report.append("æµ‹è¯•/åˆ†: æ¯åˆ†é’Ÿæ‰§è¡Œçš„æµ‹è¯•æ•°é‡")
        report.append("é¦–æ¬¡å¤±è´¥: å‘ç°ç¬¬ä¸€ä¸ªå¤±è´¥æ‰€éœ€çš„æ—¶é—´(ç§’)ï¼Œè¶Šå°è¶Šå¥½")
        report.append("å¤šæ ·æ€§: å»é‡å¤±è´¥å æ€»å¤±è´¥çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¡¨ç¤ºå¤±è´¥æ ·æœ¬è¶Šå¤šæ ·åŒ–")
        report.append("æ£€æµ‹ç‡: æˆåŠŸæ£€æµ‹åˆ°å¤±è´¥çš„æµ‹è¯•å æ¯”")
        report.append("=" * 120)

        report_text = "\n".join(report)
        print("\n" + report_text)

        with open(output_path / "comparison_report.txt", "w", encoding="utf-8") as f:
            f.write(report_text)

        print(f"  âœ… å¯¹æ¯”æŠ¥å‘Š")


if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¨ STELLAR å®éªŒå¯¹æ¯”å›¾è¡¨ç”Ÿæˆå™¨")
    print("=" * 80)

    analyzer = ExperimentAnalyzer()

    if analyzer.find_latest_results():
        print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
        analyzer.generate_comparison_plots()

        print("\n" + "=" * 80)
        print("âœ… å®Œæˆï¼ç”Ÿæˆçš„å›¾è¡¨:")
        print("   1. 1_tsr_comparison.png - TSR å¯¹æ¯”")
        print("   2. 2_time_comparison.png - æ—¶é—´å¯¹æ¯”")
        print("   3. 3_failures_over_time.png - å¤±è´¥æ›²çº¿ï¼ˆè®ºæ–‡é£æ ¼ï¼‰")
        print("   4. 4_efficiency_comparison.png - æ•ˆç‡å¯¹æ¯”")
        print("   5. 5_comprehensive_radar.png - ç»¼åˆé›·è¾¾å›¾")
        print("   6. comparison_report.txt - æ–‡æœ¬æŠ¥å‘Š")
        print("=" * 80)
    else:
        print("âŒ æœªæ‰¾åˆ°å®éªŒç»“æœ")
        print("è¯·å…ˆè¿è¡Œå®éªŒ:")
        print("  ./run_all_experiments.sh")
